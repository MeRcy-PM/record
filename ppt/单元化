@(日记)

# 容灾架构

[TOC]

基本上在所有容灾架构上都要包含以下几个通用点：
避免单点： 以上已经提到过了，单点在硬件故障，或者中间某一个服务异常时候，都有可能导致整体服务不可用。

**备注**: 数据库最后讨论，其他几种容灾架构虽然都涉及到数据库，但是数据库本身比较特殊，单独分析。

## 基础网络

### 公网网络接入

公网传输也分为三段
- 用户接入运营商，这一段基本上没有任何容灾能力，只能依靠用户切换运营商。
- 用户接入的运营商，到机房运营商中间的N跳，如果业务机房有多个运营商，则可以尝试切换运营商，如果是中间路由收敛的问题，这就比较靠天吃饭了。
- 机房接入的运营商，如果运营商本身出了问题，需要有备份的网络运营商可以切换。

### 内部专线网络

内部专线网络对于多机房的业务架构上来说，是一个平时很难注意到，但是出了问题会导致致命问题的部分。

比如对于同城双单元的模式下，一般采用DB的三机房部署，当内部专线出现问题，可能会出现脑裂，此时根据DB的实现，最坏的情况可能会出现脑裂，出现双主。即使不出现双主的问题，但是由于DB的日常运维，不同业务对应的主数据库可能存在于不同机房，当内部专线故障时，部分业务可能可以正常运作，但是部分业务可能会因为数据库不在本机房而导致无法访问，直接影响业务。

## 统一接入层

### 统一接入层及负载均衡:

基本上所有容灾架构都有基于自己的统一接入层并进行负载均衡或者流量调度，比如注明的aserver以及Ingress、HAproxy、LVS、SLB等，不同层的负载均衡有时候也会复用，比如LVS/SLB后面挂载对应nginx进行业务的负载均衡，而DNS或者类似DNS产品，比如httpDNS则会用来保障三层的负载均衡上的均衡。

![](https://raw.githubusercontent.com/MeRcy-PM/record/master/pic/%E6%8E%A5%E5%85%A5%E5%B1%82.jpg)

如上图，**暂时不考虑单元化和区域化等容灾架构下**，单纯的DNS到应用服务器的负载均衡链路一般如图，当用户访问www.aaa.com的时候，对应的DNS中会有一条记录，cname到两个不同的域名www.region1.com和www.region2.com中，两者权重相同，当然也可以调整，用于DNS的灰度。

当客户端选择了一个域名解析后，会访问对应域名的下一层，这一层可能还是一个cname，也可能是一个A记录，A记录会记录实际的三层负载均衡设备的IP，可以是一个LVS/SLB，后称VIP，虚拟IP。如图中的www.region1.com中对应了两条A记录，分别为三层负载均衡设备123.123.123.123和124.124.124.124。

当客户端选择了124.124.124.124这个VIP后，这个VIP可以绑定多个后端四层的接入设备，比如aserver，这些nginx一般称为RS，real server。这里图示为了简单，这个VIP只对应了10.123.123.122这个nginx接入层。四层接入层接入后，会根据对应调度规则，**可以是nginx内置的upstream规则调度到实际后端的业务服务器**，阿里这里一般使用vipserver key来进行后端业务服务器在统一接入层后的负载均衡，如果是无线请求会通过mtop然后用HSF来进行负载均衡。

**备注：这里的统一接入层主要写的是VIP和nginx，其实在实际网络中会比文内介绍的复杂的多，中间还有许多网关，也算是统一接入层的一部分**

### 统一接入层容灾模式

![](https://raw.githubusercontent.com/MeRcy-PM/record/master/pic/%E9%93%BE%E8%B7%AF%E5%9F%9F%E5%90%8D.png)

对于同城双活或者同城单元化或者多区域化的统一接入层容灾一般都如上图所示进行接入层的容灾。

首先会设置一个base域名，这是容灾下的统一接入域名，所有需要接入容灾域名的业务域名，只要cname到这个base域名上即可。在这个base域名后会挂载对应的链路域名，每个链路域名会指向不同机房的VIP。

以图中为例，base域名为www.aaadns.com，这个base域名下cname了两个链路域名，一个为www.os30-aaa.com，这个域名指向了两个os30机房下的VIP，另一个链路域名为www.sg52-aaa.com，这个域名指向了两个sg52机房下的VIP。业务请求域名www.aaa.com接入了这个base域名，那么这个业务域名就有了接入层的容灾能力，当两个机房都正常的时候，请求会按照比例访问两个机房。当SG52机房挂掉了，就是图中的红线，此时我们可以通过摘除base域名下的www.sg52-aaa.com来进行业务恢复，不过DNS层的恢复本身会比较慢，和TTL有关，根据测试大概需要7分钟左右。

**同城双活和同城单元化在同城的两个单元机房都挂掉以后，是无法进行容灾的，如果是多区域化的方式部署，可以将DNS从一个国家切到另一个国家来进行切流容灾，具有更强的容灾能力。**

### 统一接入层的其他工作

统一接入层除了流量调度和接入外，还会有一些其他工作：
- https证书的认证
- 新区域和VIP的建站工作
- 部分用户接入层网络加速，一种可以通过LVS跨区域挂载aserver，这种方式不太推荐，一种可以通过本区域nginx接入后跨区域调度到后端服务器，通过内网进行加速。
- 阿里目前统一接入层和安全已经剥离，因为之前waf的故障，因此aserver这里只有一个x5agent，将流量转发到霸下处置中心，进行防刷的操作。
- 第一层限流，这一层限流一般是业务2-3倍以上。

## 数据库

### 非单元化

对于常用的XDB(PolarisDB)，需要基于三副本才能进行选主，这里涉及比较深的DB知识，只简单介绍一些结论性的内容。

对于非单元化的DB部署模式，大多数使用场景为同城单单元双机房部署的业务，或者同城双单元机房部署模式，这种模式下，由于机房间的RT比较稳定也比较低，数据同步比较可靠，但是需要三个不同机房部署DB，来保证DB在主、备、log节点落在不同机房，防止单个机房故障，此时如果主节点和其中一个节点落在同一个机房，残存的节点无法独立进行选主而导致DB故障。

下图为简单的三节点三机房模式：

![](https://raw.githubusercontent.com/MeRcy-PM/record/master/pic/DB%E6%9C%BA%E6%88%BF%E5%AE%B9%E7%81%BE.png)

下图为简单的二机房部署的容灾能力：当主节点和其他节点一起挂掉的时候，剩余的一个节点无法单独进行选主。

![](https://raw.githubusercontent.com/MeRcy-PM/record/master/pic/%E4%BA%8C%E6%9C%BA%E6%88%BFDB%E5%AE%B9%E7%81%BE.png)

**非单元化的数据库会存在一个问题，当内网脑裂时，由于不同业务的不同主数据库在不同区域，可能会有跨区域访问数据库异常**

### 单元化

![](https://raw.githubusercontent.com/MeRcy-PM/record/master/pic/DB%E5%8D%95%E5%85%83%E5%8C%96.png)

按我个人理解，DB单元化主要是在异地单元化的场景下，部分业务场景属于中心型业务，而且很多业务数据都需要跨区域进行同步，因此整个部署模型类似一个卫星状，中心数据库一般也是合规中心，部署在中立国，采用三机房部署，其他区域的所有数据会使用DRC选择性同步到中心，中心也会将数据通过DRC选择性同步到其他各个区域。

## 部署模式

对于能进行同城双活或者单元化的容灾架构，为了容灾需要进行一定的冗余，最理想的情况是对等部署，这样在日常1:1的单元间流量异常时候，总会有富余的容量可以支撑切流。

如果部署模式无法做到完全对等，就需要经常进行切流演练，防止真正故障出现时候，应急策略无法使用。

之前大促完成后进行缩容后，也曾经出现过切流完以后部分应用单单元无法承载流量，直接出现限流，这对容灾是毁灭性打击。

![](https://raw.githubusercontent.com/MeRcy-PM/record/master/pic/%E8%BF%87%E5%BA%A6%E7%BC%A9%E5%AE%B9%E6%BC%94%E7%BB%83.png)

## 常见容灾架构

### 同城(单单元)双活

![](https://raw.githubusercontent.com/MeRcy-PM/record/master/pic/%E5%90%8C%E5%9F%8E%E5%8F%8C%E6%B4%BBDB.png)

同城双活是指在**一个城市即一个单元**的两个机房内部署同一个应用，比如新加坡的SG52和新加坡的OS30两个机房同时部署应用，并利用接入层和中间件的特性将两个机房内的机器都进行注册，使得两个机房的机器都可以被正常访问到。**同城双活模式主要是给以vipserver为应用入口的非无线请求使用，不过有些PC请求也会封装成无线请求，为了更好的利用单元化。**

对于aserver普通负载均衡功能而言，其vipserver模块调度上只识别单元，不识别机房，因此在调度时候只看当前vipserver返回的机器所属的单元属性，而不会查看机房属性，当两个机房的机器都注册到同一个vipserver key下，两个机房的aserver读取这个vipserver key，都会认为这些应用服务器都是可调度的，即使aserver是处在两个不同单元。

而HSF的粒度也主要是以单元为级别，两个机房内的应用隶属于同一个单元，在配置上只要配置中心化调用，就可以使下游应用都被调度到。

异步流量也类似，metaq在两个机房的部署并不影响实际流量的调度，可以出现消息生产者和消息消费者在不同机房。

因此，当某个机房出现故障的时候，vipserver和configServer是带有保活功能，可以自动摘除节点，**但是对于异步流量而言，在机房容灾上就有很大的缺陷**。

此外，对于发布类的故障，同城双活的容灾方式比较麻烦
- 需要发布工具支持以单元模式发布，如果两个单元同时发布，则切流或者应用下线拉黑杀进程的方式并不能支持快速止血。
- 对于全链路而言，同城双活的故障发生时，需要依赖很强力的定位手段，准确定位到具体应用并进行机房级别的拉黑下线杀进程，一般很难做到全链路整体进行机房级别的切流。
- 异步流量的固有问题，故障机仍然有可能生产和消费部分业务。

备注：中间件为了支持单元化，一般提供中心和单元化两种调用方式，但是在同城双活模式下一般使用中心化的方式，一个单元为一个中心配置。

### 同城单元化

![](https://raw.githubusercontent.com/MeRcy-PM/record/master/pic/%E5%8D%95%E5%85%83%E5%8C%96DB.png)

单元化是将多个机房在逻辑上拆分称为不同的逻辑单元，流量的调度统一放在统一接入层上进行处理，从统一接入层之后，尽量做到逻辑单元内的封闭。**目前主要在集团内用于无线请求**。

aserver除了同城双活欧式下的vipserver负载均衡功能外，还有一个单独的schedule模块用来做单元间调度模块，用户请求传递过来的时候，会通过http头中的一些属性把用户ID传递过来，而aserver的配置文件会解析对应的用户ID，根据一些规则计算出一个值，然后根据diamond中的不同值对应不同单元的调度逻辑将请求转发给单元化的mtop，mtop将http请求转为HSF请求，并根据HSF不同单元配置(这里一般不进行调度，所以就在不同的HSFOPS单元上注册对应单元的业务容器即可)进行业务调度，所有业务的调度规则都是在单元内封闭。

而一些异步流量的中间件，比如metaQ，或者定时任务，一般中间件本身也提供了单元化或者容灾能力，业务自行配置，来实现整个单元封闭。

当机房级别故障出现时，如果是统一接入层也有故障，可以通过链路域名直接切走来恢复故障，如果不是统一接入层的故障，而是比如某些发布或者其他因素导致的某个单元故障，则可以通过修改aserver调度的diamond进行秒级切流，整条业务线上的流量会瞬间从一个机房切换到另一个机房，也不需要进行准确定位，优先止血后修复问题。**同理，发布类问题也需要遵循一个单元发布完成后再发布另外一个单元。**

### 区域化

![](https://raw.githubusercontent.com/MeRcy-PM/record/master/pic/%E5%8C%BA%E5%9F%9F%E5%8C%96.png)

异地单元化在国际也可以成为区域化，也算异地多活，和普通单元化的思想类似，也需要根据用户ID或者一些数据来决定用户调度归属区域，和国内场景有所不同的是，用户的归属区域可能发生较大的变化，比如从一个国家到了另一个国家，这样这个用户的归属区域可能会发生变化，因此会有一个专门的应用来记录更新用户归属区域的应用，这个应用会同时更新用户归属的缓存和数据库。关于用户归属数据的记录方式也很有深度，如何节约空间。

由于区域化用户的归属区域数据可能会发生变化，因此没有在统一接入层进行处理，保证统一接入层的一致性，因此在统一接入层没有直接处理用户归属区域，而是在统一接入层后又单独接入了一层区域化路由层，这一层会处理所有用户归属，如果业务发生切流，或者用户DNS解析出现一些偏移，那区域化路由这一层会进行一次纠偏，通过注册不同区域的区域化路由应用的vipserver key进行区域间的转发。

除了区域化路由层会进行纠偏，在HSF的服务发现也会进行纠偏，不同区域间的configserver会通过skywalker进行同步，一方面是纠偏，一方面是有一些服务在某些区域没有进行部署，需要纠偏到部署的区域上。

此后的业务都在同一个区域(单元)内进行尽量封闭的完成(可能没有部署，会出现来回跨区域的纠偏)。

和同城模式不同，异地模式绕不开的一个点是部分应用的中心化，比如库存和优惠需要中心写数据库。而数据库，采用单元化的方式，中心数据库采用三地容灾部署，其他区域通过DRC来进行部分过滤(数据合规)。

区域化的切流和同城的切流不同，主要根因在于中心数据库上。切流主要区分为两种。
- 当DRC没有出现延迟的时候，数据库同步仍然正常，此时在切流的瞬间，会对要切流的用户进行时间戳操作的对比，对切流后区域的请求进行丢弃禁写，防止用户在双单元进行双写。
- 当DRC出现延迟后，比如机房级别故障，此时需要能进行在DRC延迟的情况下进行强行切流，强行切流仍然需要有禁写操作，但是可能后续需要进行部分数据的订正。

## 几种方式的对比

三种模式下面各有各的容灾能力，区域化>单元化>同城双活，但是部署和技术难度也跟这个顺序一致，需要业务根据自己的需求采用最合理的架构。

| 故障点 | 区域化 | 同城单元化 | 同城双活 | 
| -------- | -------- | --------- | ----------- |
| 区域级故障 | DNS切流到其他可用区域 | 无法容灾 | 无法容灾 |
| 单机房故障 | DNS切流到其他可用区域 | DNS切到可用区域，业务切流 | DNS切流到可用区域，业务依靠服务发现摘除异常节点 |
| 统一接入层故障 | DNS切流到其他可用区域 | DNS切到可用区域 | DNS切流到可用区域|
| 中间件故障 | 单元切流，小心失效缓存 | 单元切流，小心失效缓存 | 无法容灾，靠天吃饭 |
| 变更故障 | 按区域发布可切流 | 按单元发布可切流 | 按机房发布可切流 |
| 数据库故障 | 主库三机房，DRC延迟强切 | 三机房容灾 | 三机房容灾 |
| 中心应用故障 | 异地部署，DRC同步 | 一般不涉及 | 一般不涉及 |
| 内部专线故障 | 无DB脑裂强切 | DB脑裂无法切流 | DB脑裂无法切流 |
| 切流时配置下发功能故障 | - | - | - |